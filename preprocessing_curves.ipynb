{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Alex Cohen Dambr√≥s Lopes \n",
    "\n",
    "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Getting all light curves from space telescopes and running their pre-processing and data analysis\n",
    "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "This is an example code used to obtain light curves from space telescopes and preprocess \n",
    "them automatically\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import lightkurve as lk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from Functions import all_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_labels(ax, spacing=5):\n",
    "    # For each bar: Place a label\n",
    "    for rect in ax.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.0f}\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        ax.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= RandomState =============\n",
    "random_state = np.random.RandomState(123)\n",
    "\n",
    "\n",
    "# ============= Warnings =============\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # show all cols\n",
    "pd.set_option('display.max_rows', None) # show all rows\n",
    "pd.set_option(\"expand_frame_repr\", False) # print cols side by side as it's supposed to be"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_functions.download_all_datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tess = all_functions.read_dataset('tess')\n",
    "df_kepler = all_functions.read_dataset('kepler')\n",
    "df_k2 = all_functions.read_dataset('k2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kepler.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Rename specific columns\n",
    "df_tess.rename(columns={\"tfopwg_disp\": \"disposition\"}, inplace = True)\n",
    "df_kepler.rename(columns={\"koi_disposition\": \"disposition\"}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telescope_data_list = {\"TESS\": df_tess, \"KEPLER\": df_kepler, \"K2\": df_k2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "Total number of instances and columns\n",
    "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\"\"\"\n",
    "\n",
    "list_disposition = ['FALSE POSITIVE', 'CONFIRMED']\n",
    "\n",
    "for telescope, df_telescope in telescope_data_list.items():\n",
    "    print(f\"============ {telescope} ============\")\n",
    "    print(f\"For the data collected by the {telescope} space telescope, it has {df_telescope.shape[1]} columns and {df_telescope.shape[0]} instances.\")\n",
    "    \n",
    "    \"\"\"\n",
    "    =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "    Percentage of false positives and candidates\n",
    "    =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "    \"\"\"\n",
    "    \n",
    "    df_filtered = df_telescope[(df_telescope['disposition'] == 'FALSE POSITIVE') | (df_telescope['disposition'] == 'CONFIRMED')]\n",
    "    sum_false_positives = (df_filtered['disposition'] == 'FALSE POSITIVE').sum()\n",
    "    sum_confirmed = (df_filtered['disposition'] == 'CONFIRMED').sum()\n",
    "    print(\"Total False Positives: %d, Total Confirmed: %d, Others: %d\" % (sum_false_positives, sum_confirmed, df_telescope.shape[0] - (sum_confirmed + sum_false_positives)))\n",
    "    print(\"Percentage False Positives: %.2f%%, Percentage Confirmed: %.2f%%, Percentage of Others: %.2f%%\" % ((sum_false_positives / df_telescope.shape[0]) * 100, (sum_confirmed / df_telescope.shape[0]) * 100, (df_telescope.shape[0] - (sum_confirmed + sum_false_positives)) * 100 / df_telescope.shape[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESS\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(x= df_tess[\"disposition\"], color = \"#0092D4\", ax=ax)\n",
    "add_value_labels(ax)\n",
    "plt.title(\"Amount of disposition of the data present in the TESS telescope\", loc=\"left\")\n",
    "plt.xlabel(\"Disposition\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.ylim(0, 5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEPLER\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(x= df_kepler[\"disposition\"], color = \"#0092D4\", ax=ax)\n",
    "add_value_labels(ax)\n",
    "plt.title(\"Amount of disposition of the data present in the KEPLER telescope\", loc=\"left\")\n",
    "plt.xlabel(\"Disposition\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.ylim(0, 5200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K2\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(x= df_k2[\"disposition\"], color = \"#0092D4\", ax=ax)\n",
    "add_value_labels(ax)\n",
    "plt.title(\"Amount of disposition of the data present in the K2 telescope\", loc=\"left\")\n",
    "plt.xlabel(\"Disposition\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.ylim(0, 2500)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting specific columns for the first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows of planets that were discovered by methods other than transit\n",
    "df_k2 = df_k2[df_k2['discoverymethod'] != 'Radial Velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESS\n",
    "df_tess  = df_tess[['tid', 'disposition', 'pl_orbper', 'pl_trandurh']]\n",
    "df_tess.rename(columns={\"tid\": \"id_target\", \"pl_orbper\": \"period\", \"pl_trandurh\": \"duration\"}, inplace = True)\n",
    "\n",
    "# KEPLER\n",
    "df_kepler = df_kepler[['kepid', 'disposition','koi_period','koi_duration', 'koi_time0bk']]\n",
    "df_kepler.rename(columns={\"kepid\": \"id_target\", \"koi_period\": \"period\", \"koi_duration\": \"duration\"}, inplace = True)\n",
    "\n",
    "# K2\n",
    "df_k2 = df_k2[['tic_id', 'disposition', 'pl_orbper', 'pl_trandur']]\n",
    "df_k2.rename(columns={\"tic_id\": \"id_target\", \"pl_orbper\": \"period\", \"pl_trandur\": \"duration\"}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local View & Global View\n",
    "\n",
    "- global view -> All light curves are binned to the same length\n",
    "\n",
    "- local view -> so that the transit occupies a fixed fraction of the resulting vector.\n",
    "\n",
    "The global view has a fixed length of 2001 bins, while the local view has a fixed length of 201 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_disposition = ['CONFIRMED', 'FALSE POSITIVE']\n",
    "\n",
    "\n",
    "# filtering the data by confirmed targets and false positives\n",
    "df_tess = df_tess[df_tess['disposition'].isin(candidate_disposition)]\n",
    "df_kepler = df_kepler[df_kepler['disposition'].isin(candidate_disposition)]\n",
    "df_k2 = df_k2[df_k2['disposition'].isin(candidate_disposition)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telescopes_list = {'Kepler': df_kepler.sample(1), 'TESS': df_tess.sample(1)}\n",
    "telescopes_list = {'Kepler': df_kepler.head(1)} # TEST\n",
    "\n",
    "\n",
    "local_curves = []\n",
    "local_targets = []\n",
    "\n",
    "global_curves = []\n",
    "global_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_telescope, df_telescope in telescopes_list.items():\n",
    "    print(\"Telescope:\", name_telescope)\n",
    "    for index, row in df_telescope.iterrows():\n",
    "        try:\n",
    "            id_target, period, duration, t0 = row[0], row[2], row[3], row[4]\n",
    "        except:\n",
    "            id_target, period, duration = row[0], row[2], row[3]\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            if name_telescope == 'Kepler':\n",
    "                id_target = 'KIC ' + str(id_target)\n",
    "                lcs = lk.search_lightcurve(id_target, author= name_telescope, cadence='long').download_all()\n",
    "                \n",
    "            elif name_telescope == 'TESS':\n",
    "                id_target = 'TIC ' + str(id_target)\n",
    "                lcs = lk.search_lightcurve(id_target, mission= name_telescope, cadence='long').download_all()\n",
    "            \n",
    "            if not(lcs is None):\n",
    "                \n",
    "                # This method concatenates all quarters in our LightCurveCollection together, and normalizes them at the same time.\n",
    "                lc_raw = lcs.stitch()\n",
    "                \n",
    "                # Clean outliers, but only those that are above the mean level (e.g. attributable to stellar flares or cosmic rays).\n",
    "                lc_clean = lc_raw.remove_outliers(sigma=3)\n",
    "                \n",
    "                # We have to mask the transit to avoid self-subtraction the genuine planet signal when we flatten the lightcurve. We have to do a hack to find where the time series should be masked.\n",
    "                if t0 != None:\n",
    "                    temp_fold = lc_clean.fold(period, epoch_time=t0)\n",
    "                else:\n",
    "                    temp_fold = lc_clean.fold(period)\n",
    "            \n",
    "                fractional_duration = (duration / 24.0) / period\n",
    "                phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
    "                transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
    "\n",
    "                lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n",
    "                \n",
    "                # Now fold the cleaned, flattened lightcurve:\n",
    "                if t0 != None:\n",
    "                    lc_fold = lc_flat.fold(period, epoch_time=t0)\n",
    "                else:\n",
    "                    lc_fold = lc_flat.fold(period)\n",
    "                    \n",
    "                # ========= Defining global curves =========\n",
    "                lc_global = lc_fold.bin(bins=2001).normalize() - 1\n",
    "                lc_global = (lc_global / np.abs(lc_global.flux.min()) ) * 2.0 + 1\n",
    "\n",
    "                phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n",
    "                lc_zoom = lc_fold[phase_mask]\n",
    "\n",
    "\n",
    "                # ========= Defining local curves =========\n",
    "                lc_local = lc_zoom.bin(bins=201).normalize() - 1\n",
    "                lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n",
    "        \n",
    "                local_targets.append(row[1])\n",
    "                local_curves.append(lc_local.flux.value)\n",
    "                \n",
    "                global_targets.append(row[1])\n",
    "                global_curves.append(lc_global.flux.value)\n",
    "                \n",
    "                print(f\"{id_target} target pre-processing performed, Disposition:{row[1]}\")\n",
    "            \n",
    "            else:\n",
    "                print(\"Error downloading target data:\", id_target)\n",
    "        \n",
    "        except Exception as error:\n",
    "            print(f\"Failed at id: {id_target} | Error: {error}\")\n",
    "            \n",
    "            \n",
    "df_local = pd.DataFrame(local_curves)\n",
    "df_global = pd.DataFrame(global_curves)\n",
    "\n",
    "df_global = df_global.interpolate(axis=1)\n",
    "df_local = df_local.interpolate(axis=1)\n",
    "\n",
    "df_global['label'] = pd.Series(global_targets)\n",
    "df_local['label'] = pd.Series(local_targets)\n",
    "\n",
    "get_current_path = os.getcwd()\n",
    "\n",
    "# Path\n",
    "path = os.path.join(get_current_path, 'Preprocessed')\n",
    "\n",
    "# ============= Create the directory =============\n",
    "try:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "    else:\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "    \n",
    "except OSError as error:\n",
    "    print(\"Directory can not be created: \", error)\n",
    "    \n",
    "df_global.to_csv(path+ '\\\\preprocessed_local_view.csv')  \n",
    "df_local.to_csv(path+ '\\\\preprocessed_global_view.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "Important !!!\n",
    "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "The 'bin' function present in the current version of the lightkurve 2.3.0 library has an object type error for this reason \n",
    "it is necessary to replace the function in the open source library for the code to work correctly! \n",
    "\n",
    "This function below is the function present in previous versions of the library. \n",
    "It is not possible to go back to old versions of the package in general because there are functions that were not implemented before.\n",
    "\n",
    "\n",
    "def bin(\n",
    "        self,\n",
    "        time_bin_size=None,\n",
    "        time_bin_start=None,\n",
    "        n_bins=None,\n",
    "        aggregate_func=None,\n",
    "        bins=None,\n",
    "        binsize=None,\n",
    "    ):\n",
    "    \n",
    "        if binsize is not None and bins is not None:\n",
    "            raise ValueError(\"Only one of ``bins`` and ``binsize`` can be specified.\")\n",
    "        elif (binsize is not None or bins is not None) and (\n",
    "            time_bin_size is not None or n_bins is not None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"``bins`` or ``binsize`` conflicts with \"\n",
    "                \"``n_bins`` or ``time_bin_size``.\"\n",
    "            )\n",
    "        elif bins is not None:\n",
    "            if np.array(bins).dtype != np.int:\n",
    "                raise TypeError(\"``bins`` must have integer type.\")\n",
    "            elif np.size(bins) != 1:\n",
    "                raise ValueError(\"``bins`` must be a single number.\")\n",
    "\n",
    "        if time_bin_start is None:\n",
    "            time_bin_start = self.time[0]\n",
    "        if not isinstance(time_bin_start, (Time, TimeDelta)):\n",
    "            if isinstance(self.time, TimeDelta):\n",
    "                time_bin_start = TimeDelta(\n",
    "                    time_bin_start, format=self.time.format, scale=self.time.scale\n",
    "                )\n",
    "            else:\n",
    "                time_bin_start = Time(\n",
    "                    time_bin_start, format=self.time.format, scale=self.time.scale\n",
    "                )\n",
    "\n",
    "        # Backwards compatibility with Lightkurve v1.x\n",
    "        if time_bin_size is None:\n",
    "            if bins is not None:\n",
    "                i = len(self.time) - np.searchsorted(\n",
    "                    self.time.value, time_bin_start.value - 1e-10\n",
    "                )\n",
    "                time_bin_size = (\n",
    "                    (self.time[-1] - time_bin_start) * i / ((i - 1) * bins)\n",
    "                ).to(u.day)\n",
    "            elif binsize is not None:\n",
    "                i = np.searchsorted(self.time.value, time_bin_start.value - 1e-10)\n",
    "                time_bin_size = (self.time[i + binsize] - self.time[i]).to(u.day)\n",
    "            else:\n",
    "                time_bin_size = 0.5 * u.day\n",
    "        if not isinstance(time_bin_size, Quantity):\n",
    "            time_bin_size *= u.day\n",
    "\n",
    "        # Call AstroPy's aggregate_downsample\n",
    "        with warnings.catch_warnings():\n",
    "            # ignore uninteresting empty slice warnings\n",
    "            warnings.simplefilter(\"ignore\", (RuntimeWarning, AstropyUserWarning))\n",
    "            ts = aggregate_downsample(\n",
    "                self,\n",
    "                time_bin_size=time_bin_size,\n",
    "                n_bins=n_bins,\n",
    "                time_bin_start=time_bin_start,\n",
    "                aggregate_func=aggregate_func,\n",
    "            )\n",
    "\n",
    "            # If `flux_err` is populated, assume the errors combine as the root-mean-square\n",
    "            if np.any(np.isfinite(self.flux_err)):\n",
    "                rmse_func = (\n",
    "                    lambda x: np.sqrt(np.nansum(x ** 2)) / len(np.atleast_1d(x))\n",
    "                    if np.any(np.isfinite(x))\n",
    "                    else np.nan\n",
    "                )\n",
    "                ts_err = aggregate_downsample(\n",
    "                    self,\n",
    "                    time_bin_size=time_bin_size,\n",
    "                    n_bins=n_bins,\n",
    "                    time_bin_start=time_bin_start,\n",
    "                    aggregate_func=rmse_func,\n",
    "                )\n",
    "                ts[\"flux_err\"] = ts_err[\"flux_err\"]\n",
    "            # If `flux_err` is unavailable, populate `flux_err` as nanstd(flux)\n",
    "            else:\n",
    "                ts_err = aggregate_downsample(\n",
    "                    self,\n",
    "                    time_bin_size=time_bin_size,\n",
    "                    n_bins=n_bins,\n",
    "                    time_bin_start=time_bin_start,\n",
    "                    aggregate_func=np.nanstd,\n",
    "                )\n",
    "                ts[\"flux_err\"] = ts_err[\"flux\"]\n",
    "\n",
    "        # Prepare a LightCurve object by ensuring there is a time column\n",
    "        ts._required_columns = []\n",
    "        ts.add_column(ts.time_bin_start + ts.time_bin_size / 2.0, name=\"time\")\n",
    "\n",
    "        # Ensure the required columns appear in the correct order\n",
    "        for idx, colname in enumerate(self.__class__._required_columns):\n",
    "            tmpcol = ts[colname]\n",
    "            ts.remove_column(colname)\n",
    "            ts.add_column(tmpcol, name=colname, index=idx)\n",
    "\n",
    "        return self.__class__(ts, meta=self.meta) \n",
    "        \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2aad3367d6b78e24ba4ea93151d29e7ae5b13b686914646e5d7fb2c62a3fe26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
